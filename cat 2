
from sympy import Symbol, diff

# Define a symbolic variable
x = Symbol('x')

# Define a function
def f(x):
  return x**2 + 3*x - 4

# Differentiate the function
df = diff(f(x), x)




# Print the derivative
print(df)
def f(x):  # Define the function to integrate
  return x**2

def trapezoidal_rule(f, a, b, n):
  """
  This function computes the numerical integral of f(x) from a to b using the Trapezoidal Rule.

  Args:
      f: The function to integrate.
      a: The lower bound of integration.
      b: The upper bound of integration.
      n: The number of subintervals.

  Returns:
      The approximate value of the integral.
  """
  h = (b - a) / n  # Calculate the width of each subinterval
  integral = 0.5 * (f(a) + f(b))  # Add contributions from endpoints

  # Loop through interior points and add their contribution
  for i in range(1, n):
    x = a + i * h
    integral += f(x)

  return integral * h  # Multiply by width to get total area
a = 0
b = 1
n = 100  

result = trapezoidal_rule(f, a, b, n)

print("The approximate integral of f(x) from", a, "to", b, "using", n, "sub-intervals is:", result)



import numpy as np
from scipy.optimize import curve_fit

# Define the function to fit (example: exponential function)
def exp_func(x, a, b):
  return a * np.exp(b * x)

# Generate some sample data
x = np.linspace(0, 5, 100)  
y = exp_func(x, 2, -1)  
y += np.random.randn(len(y)) * 0.2

# Popt holds the optimized parameters, pcov is the covariance matrix
popt, pcov = curve_fit(exp_func, x, y)

# Print the optimized parameters
print("Optimized parameters:", popt)


fitted_curve = exp_func(x, *popt)  

# Plot the data and the fitted curve
import matplotlib.pyplot as plt

plt.plot(x, y, 'o', label='Data')
plt.plot(x, fitted_curve, label='Fitted Curve')
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.show()




import numpy as np
from sklearn.linear_model import LinearRegression

# Sample data (replace with your own data)
x = np.array([10,13,15,17,19])
y = np.array([2, 4, 5, 4, 5])

# Create a linear regression model
model = LinearRegression()

# Fit the model to the data
model.fit(x.reshape(-1, 1), y)  # Reshape x for compatibility

# Print the coefficients (slope and intercept)
print("Coefficients:", model.coef_)
print("Intercept:", model.intercept_)

# Make predictions
predicted_y = model.predict(np.array([6]))  # Predict for a new data point
print("Predicted value for x=6:", predicted_y[0])



import numpy as np
from scipy.interpolate import CubicSpline

# Sample data (replace with your own data)
x = np.array([0, 1, 2, 3])
y = np.array([1, 2, 1, 3])

# Create a cubic spline object
spline = CubicSpline(x, y)

# Evaluate the spline at new points
new_x = np.linspace(0, 3, 100)  # Create dense points for smooth curve
new_y = spline(new_x)

# Plot the data points and the interpolated curve
import matplotlib.pyplot as plt

plt.plot(x, y, 'o', label='Data Points')
plt.plot(new_x, new_y, label='Spline Interpolation')
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.show()

# Get the interpolated value at a specific point
specific_x = 1.5
interpolated_y = spline(specific_x)

print("Interpolated value at x =", specific_x, ":", interpolated_y)


import numpy as np

def newton_divided_difference(x, y):
    """
    Implement Newton's Divided Difference method for polynomial interpolation.
    
    Parameters:
    x: list or array of x coordinates
    y: list or array of y coordinates
    
    Returns:
    coefficients: list of coefficients for the interpolating polynomial
    """
    n = len(x)
    coefficients = [y[0]]  # The first coefficient is always y[0]
    
    # Create a matrix to store divided differences
    divided_diff = np.zeros((n, n))
    divided_diff[:, 0] = y
    
    for j in range(1, n):
        for i in range(n - j):
            divided_diff[i][j] = (divided_diff[i+1][j-1] - divided_diff[i][j-1]) / (x[i+j] - x[i])
    
    # The coefficients are the first row of the divided difference matrix
    coefficients = [divided_diff[0][j] for j in range(n)]
    
    return coefficients

def newton_polynomial(coeffs, x_data, x):
    """
    Evaluate the Newton polynomial at point x.
    
    Parameters:
    coeffs: coefficients from newton_divided_difference
    x_data: original x points
    x: point at which to evaluate the polynomial
    
    Returns:
    y: value of the polynomial at x
    """
    n = len(x_data) - 1
    p = coeffs[n]
    for k in range(1, n + 1):
        p = coeffs[n-k] + (x - x_data[n-k])*p
    return p

# Given data points
x = [1, 2, 3, 4]
y = [1, 4, 9, 16]

# Compute the coefficients
coeffs = newton_divided_difference(x, y)

print("Coefficients of the Newton polynomial:")
for i, coeff in enumerate(coeffs):
    print(f"a{i}: {coeff}")

# Verify the interpolation
print("\nVerifying interpolation:")
for xi, yi in zip(x, y):
    y_interp = newton_polynomial(coeffs, x, xi)
    print(f"f({xi}) = {y_interp:.6f}, Original y = {yi}")

# Interpolate at a new point
x_new = 2.5
y_new = newton_polynomial(coeffs, x, x_new)
print(f"\nInterpolated value at x = {x_new}: {y_new:.6f}")





import numpy as np
from scipy.interpolate import CubicSpline

# Sample data (replace with your own data)
x = np.array([0, 1, 2, 3])
y = np.array([1, 2, 1, 3])

# Create a cubic spline object
spline = CubicSpline(x, y)

# Evaluate the spline at new points
new_x = np.linspace(0, 3, 100)  # Create dense points for smooth curve
new_y = spline(new_x)

# Plot the data points and the interpolated curve
import matplotlib.pyplot as plt

plt.plot(x, y, 'o', label='Data Points')
plt.plot(new_x, new_y, label='Spline Interpolation')
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.show()

# Get the interpolated value at a specific point
specific_x = 1.5
interpolated_y = spline(specific_x)

print("Interpolated value at x =", specific_x, ":", interpolated_y)



import numpy as np

def newton_divided_difference(x, y):
    """
    Implement Newton's Divided Difference method for polynomial interpolation.
    
    Parameters:
    x: list or array of x coordinates
    y: list or array of y coordinates
    
    Returns:
    coefficients: list of coefficients for the interpolating polynomial
    """
    n = len(x)
    coefficients = [y[0]]  # The first coefficient is always y[0]
    
    # Create a matrix to store divided differences
    divided_diff = np.zeros((n, n))
    divided_diff[:, 0] = y
    
    for j in range(1, n):
        for i in range(n - j):
            divided_diff[i][j] = (divided_diff[i+1][j-1] - divided_diff[i][j-1]) / (x[i+j] - x[i])
    
    # The coefficients are the first row of the divided difference matrix
    coefficients = [divided_diff[0][j] for j in range(n)]
    
    return coefficients

def newton_polynomial(coeffs, x_data, x):
    """
    Evaluate the Newton polynomial at point x.
    
    Parameters:
    coeffs: coefficients from newton_divided_difference
    x_data: original x points
    x: point at which to evaluate the polynomial
    
    Returns:
    y: value of the polynomial at x
    """
    n = len(x_data) - 1
    p = coeffs[n]
    for k in range(1, n + 1):
        p = coeffs[n-k] + (x - x_data[n-k])*p
    return p

# Given data points
x = [1, 2, 3, 4]
y = [1, 4, 9, 16]

# Compute the coefficients
coeffs = newton_divided_difference(x, y)

print("Coefficients of the Newton polynomial:")
for i, coeff in enumerate(coeffs):
    print(f"a{i}: {coeff}")

# Verify the interpolation
print("\nVerifying interpolation:")
for xi, yi in zip(x, y):
    y_interp = newton_polynomial(coeffs, x, xi)
    print(f"f({xi}) = {y_interp:.6f}, Original y = {yi}")

# Interpolate at a new point
x_new = 2.5
y_new = newton_polynomial(coeffs, x, x_new)
print(f"\nInterpolated value at x = {x_new}: {y_new:.6f}")



import numpy as np

def newton_divided_difference(x, y):
    """
    Implement Newton's Divided Difference method for polynomial interpolation.
    
    Parameters:
    x: list or array of x coordinates
    y: list or array of y coordinates
    
    Returns:
    coefficients: list of coefficients for the interpolating polynomial
    """
    n = len(x)
    coefficients = [y[0]]  # The first coefficient is always y[0]
    
    # Create a matrix to store divided differences
    divided_diff = np.zeros((n, n))
    divided_diff[:, 0] = y
    
    for j in range(1, n):
        for i in range(n - j):
            divided_diff[i][j] = (divided_diff[i+1][j-1] - divided_diff[i][j-1]) / (x[i+j] - x[i])
    
    # The coefficients are the first row of the divided difference matrix
    coefficients = [divided_diff[0][j] for j in range(n)]
    
    return coefficients

def newton_polynomial(coeffs, x_data, x):
    """
    Evaluate the Newton polynomial at point x.
    
    Parameters:
    coeffs: coefficients from newton_divided_difference
    x_data: original x points
    x: point at which to evaluate the polynomial
    
    Returns:
    y: value of the polynomial at x
    """
    n = len(x_data) - 1
    p = coeffs[n]
    for k in range(1, n + 1):
        p = coeffs[n-k] + (x - x_data[n-k])*p
    return p

# Given data points
x = [1, 2, 3, 4]
y = [1, 4, 9, 16]

# Compute the coefficients
coeffs = newton_divided_difference(x, y)

print("Coefficients of the Newton polynomial:")
for i, coeff in enumerate(coeffs):
    print(f"a{i}: {coeff}")

# Verify the interpolation
print("\nVerifying interpolation:")
for xi, yi in zip(x, y):
    y_interp = newton_polynomial(coeffs, x, xi)
    print(f"f({xi}) = {y_interp:.6f}, Original y = {yi}")

# Interpolate at a new point
x_new = 2.5
y_new = newton_polynomial(coeffs, x, x_new)
print(f"\nInterpolated value at x = {x_new}: {y_new:.6f}")





import numpy as np
import matplotlib.pyplot as plt

def analyze_signal_fft(f1, f2, fs, duration):
    # Create time array
    t = np.linspace(0, duration, int(fs * duration), endpoint=False)
    
    # Generate the signal
    s = np.sin(2 * np.pi * f1 * t) + np.sin(2 * np.pi * f2 * t)
    
    # Compute the FFT
    fft_result = np.fft.fft(s)
    
    # Compute the corresponding frequencies
    freqs = np.fft.fftfreq(len(t), 1/fs)
    
    # Compute the magnitude spectrum
    magnitude_spectrum = np.abs(fft_result)
    
    # Plot the results
    plt.figure(figsize=(12, 6))
    plt.plot(freqs[:len(freqs)//2], magnitude_spectrum[:len(freqs)//2])
    plt.title('Magnitude Spectrum of the Signa



def f(x):  # Define the function to integrate (replace with your own function)
  return x**2

def trapezoidal_rule(f, a, b, n):
  """
  This function computes the numerical integral of f(x) from a to b using the Trapezoidal Rule.

  Args:
      f: The function to integrate.
      a: The lower bound of integration.
      b: The upper bound of integration.
      n: The number of subintervals.

  Returns:
      The approximate value of the integral.
  """
  h = (b - a) / n  
  integral = 0.5 * (f(a) + f(b))  # Add contributions from endpoints

  for i in range(1, n):
    x = a + i * h
    integral += f(x)

  return integral * h  # Multiply by width to get total area

a = 0
b = 1
n = 1000  # Number of sub-intervals (more sub-intervals lead to higher accuracy)

result = trapezoidal_rule(f, a, b, n)

print("The approximate integral of f(x) from", a, "to", b, "using", n, "subintervals is:", result)




import numpy as np

def lagrange_polynomial(x, y):
    n = len(x)
    
    def L(k, x):
        """Compute the k-th Lagrange polynomial at x."""
        L = np.ones_like(x)
        for j in range(n):
            if j != k:
                L *= (x - x[j]) / (x[k] - x[j])
        return L
    
    def P(x):
        """Compute the Lagrange interpolation polynomial at x."""
        return sum(y[k] * L(k, x) for k in range(n))
    
    # Compute coefficients by evaluating P at a set of points
    x_eval = np.linspace(min(x), max(x), 100)
    y_eval = P(x_eval)
    
    # Fit a polynomial to these points to get the coefficients
    coeffs = np.polyfit(x_eval, y_eval, n-1)
    
    return coeffs[::-1]  # Reverse to get ascending order of powers

# Given data points
x = np.array([1, 2, 3, 4])
y = np.array([1, 4, 9, 16])

# Compute the coefficients
coefficients = lagrange_polynomial(x, y)

print("Coefficients of the Lagrange polynomial (in ascending order of powers):")
for i, coeff in enumerate(coefficients):
    print(f"x^{i}: {coeff}")

def polynomial(x, coeffs):
    return sum(coeff * x**i for i, coeff in enumerate(coeffs))

for xi, yi in zip(x, y):
    print(f"f({xi}) = {polynomial(xi, coefficients):.6f}, Original y = {yi}")



import numpy as np

def power_iteration(A, num_iterations=1000, tolerance=1e-9):
    n, _ = A.shape
    b_k = np.random.rand(n)
    
    for _ in range(num_iterations):
        # Calculate the matrix-by-vector product Ab
        b_k1 = np.dot(A, b_k)
        
        # Calculate the norm
        b_k1_norm = np.linalg.norm(b_k1)
        
        # Re normalize the vector
        b_k = b_k1 / b_k1_norm
        
        # Check for convergence
        if np.linalg.norm(np.dot(A, b_k) - b_k1_norm * b_k) < tolerance:
            break
    
    eigenvalue = np.dot(b_k.T, np.dot(A, b_k)) / np.dot(b_k.T, b_k)
    eigenvector = b_k
    
    return eigenvalue, eigenvector

# Define the matrix
A = np.array([[4, 1, 1], 
              [1, 3, -1], 
              [1, -1, 2]])

# Compute the dominant eigenvalue and eigenvector
eigenvalue, eigenvector = power_iteration(A)
print("Dominant Eigenvalue:", eigenvalue)
print("Corresponding Eigenvector:", eigenvector)




def qr_algorithm(A, num_iterations=1000, tolerance=1e-9):
    n, _ = A.shape
    Q = np.eye(n)
    R = A.copy()
    
    for _ in range(num_iterations):
        Q, R = np.linalg.qr(R @ Q)
        
        if np.allclose(R @ Q - Q @ np.diag(np.diag(R)), np.zeros((n, n)), atol=tolerance):
            break
    
    eigenvalues = np.diag(R)
    eigenvectors = Q
    
    return eigenvalues, eigenvectors

# Compute all eigenvalues and eigenvectors
eigenvalues, eigenvectors = qr_algorithm(A)
print("Eigenvalues:", eigenvalues)
print("Eigenvectors:\n", eigenvectors)
